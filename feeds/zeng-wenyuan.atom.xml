<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>观测站</title><link href="https://zwy1135.github.io/" rel="alternate"></link><link href="https://zwy1135.github.io/feeds%5Czeng-wenyuan.atom.xml" rel="self"></link><id>https://zwy1135.github.io/</id><updated>2014-08-20T00:00:00+08:00</updated><entry><title>玩转Kaggle之Titanic(3):该用哪个特征啊？</title><link href="https://zwy1135.github.io/wan-zhuan-kagglezhi-titanic3gai-yong-na-ge-te-zheng-a.html" rel="alternate"></link><updated>2014-08-20T00:00:00+08:00</updated><author><name>Zeng Wenyuan</name></author><id>tag:zwy1135.github.io,2014-08-20:wan-zhuan-kagglezhi-titanic3gai-yong-na-ge-te-zheng-a.html</id><summary type="html">&lt;p&gt;接上回。&lt;/p&gt;
&lt;p&gt;上回说到我们准备好了数据，可以进行特征提取了。&lt;/p&gt;
&lt;p&gt;呃，其实这里是我用词不当，特征提取(Feature extraction)是从不规则数据中提取出特征，转化为我们的分类期可以用的形式。比如说将自然文本提取词频啦，词语的前后关系啦……做成类似我们使用的这种表格的形式。这也是可以再开一个系列讲的东西。这里也不赘述了。&lt;/p&gt;
&lt;p&gt;这一篇我想讲的其实是特征选取(Feature selection)。换句话说就是计算那些东西该输进分类器里，哪些该扔掉。&lt;/p&gt;
&lt;p&gt;为什么要选取特征？把所有的数据都输进分类器里让他自己训练不好么？&lt;/p&gt;
&lt;p&gt;还真不好。&lt;/p&gt;
&lt;p&gt;因为，数据维度加大也就是数据列越多会导致分类期的训练时间飞速上涨。&lt;/p&gt;
&lt;p&gt;我们这回用的数据就那么几列，怎样都无所谓了。但是给你一段淘宝啊亚马逊啊的交易数据或者人类遗传编码数据呢，那可是轻轻松松破百万列的数据啊。要是都扔进分类器里应算，那你就做好准备算到天荒地老吧。(去超算中心一包就包年的土豪们，请自动无视这段话。)&lt;/p&gt;
&lt;p&gt;另外，将多余数据加入到分类器也会增加过度拟合(over-fitting)的风险。&lt;/p&gt;
&lt;p&gt;什么是过度拟合呢？简单来说就是分类器计算出来的规则太过于契合训练集导致它在训练集外的效果下降。再举个例子，假如现在有一个判断性别的任务，给的数据有Y染色体数目，发色，头发长度，肤色，指甲长度。正确情况下，分类器应该只根据Y染色体数目判断性别。但一旦分类器过拟合了，它就可能拟合出Y染色体数目不为零，头发长度小于10 CM，发色与肤色都不为白色者为男性这种规则出来。&lt;/p&gt;
&lt;p&gt;说了这么多，到底我们应该怎么做特征提取啊？&lt;/p&gt;
&lt;p&gt;两个思路(其实都差不多了):掐头，去尾。&lt;/p&gt;
&lt;h4&gt;掐头&lt;/h4&gt;
&lt;p&gt;所谓掐头就是说提取出重要程度最高的特征。&lt;/p&gt;
&lt;p&gt;怎么确认特征的重要性呢，那就要看各位的统计功底了，你可以把各特征与标签分别作卡方检验、F检验、p值检验等。甚至是拿一部分训练数据直接丢进分类器里训练，然后检查分类器对各特征的重要性评估数据。&lt;/p&gt;
&lt;p&gt;对于这种思路Scikit-learn 提供了&lt;a href="http://scikit-learn.org/stable/modules/feature_selection.html"&gt;好几种方法&lt;/a&gt;。比如说选取最好的k个特征&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest"&gt;SelectKBest&lt;/a&gt;，选取最好的百分之p的特征&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html#sklearn.feature_selection.SelectPercentile"&gt;SelectPercentile&lt;/a&gt;等等。&lt;/p&gt;
&lt;h4&gt;去尾&lt;/h4&gt;
&lt;p&gt;所谓去尾就是将不重要的特征砍掉。&lt;/p&gt;
&lt;p&gt;什么是不重要的特征呢？还拿上面判断性别的任务来说，假如现在你的数据里有一列是眼睛的数量。这个量在每一条数据里都为2，你认为这个量重不重要呢？换句话说，变化率在某一个值以下的特征是不重要的。(这个判断隐含了一个前提条件，即训练集中每一个分类的数量都是足量的，如果每一类都足量的前提下一个变量的变化率还是很低，那么它就应该和每一个类都没关系)&lt;/p&gt;
&lt;p&gt;在scikit-learn里面这个思路是由 VarianceThreshold 实现的。(居然这个函数不在API文档里的-_-!,大家到源码里面去看吧)&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;在这里，我们用的数据也就那么几列，掐完头就没什么数据了，所以我就只做了去尾的工作来排除无效数据。代码如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.feature_selection&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;VarianceThreshold&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;selection&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;selector&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;VarianceThreshold&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;threshold&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;selector&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;selector&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;hr /&gt;
&lt;p&gt;另外呢，从广义上来说，特征选取也是一个数据降维的过程。所以也可以用PCA这类的方法来做。有兴趣的同学可以看一看这个&lt;a href="scikit-learn.org/stable/modules/decomposition.html"&gt;链接&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Written with &lt;a href="https://stackedit.io/"&gt;StackEdit&lt;/a&gt;.&lt;/p&gt;</summary><category term="Machine Learning"></category><category term="Scikit-Learn"></category><category term="python"></category></entry><entry><title>转投python3了</title><link href="https://zwy1135.github.io/zhuan-tou-python3liao.html" rel="alternate"></link><updated>2014-08-19T00:00:00+08:00</updated><author><name>Zeng Wenyuan</name></author><id>tag:zwy1135.github.io,2014-08-19:zhuan-tou-python3liao.html</id><summary type="html">&lt;p&gt;这几天关注了一下 python3 的兼容性情况，发现我平时常用的包都早就兼容 python3 了。又想起前段时间折腾的要死的 python2 编码问题，总觉的是时候拥抱新技术了。于是呢这一篇博客就成了我用 python3 版的 pelican 编译的第一篇博客了，可喜可贺，特此留念。&lt;/p&gt;
&lt;p&gt;另外这几天发现了一个叫 conda 的 python 包管理器，甚是好用啊。最主要的是它用二进制安装包的形式来管理各种 python 包，而不是像 pip 一样管理源码包。这对于像我这种主要使用 windows 的家伙简直是天大的福音啊。从此更新 numpy 和 scipy 再也不用自己去下安装包了。再也不用折腾 lapack 和 blas 这种蛋疼得问题库了。&lt;/p&gt;
&lt;p&gt;Written with &lt;a href="https://stackedit.io/"&gt;StackEdit&lt;/a&gt;.&lt;/p&gt;</summary><category term="python"></category><category term="conda"></category></entry><entry><title>玩转Kaggle之Titanic(2):好大一坨数据啊</title><link href="https://zwy1135.github.io/wan-zhuan-kagglezhi-titanic2hao-da-yi-tuo-shu-ju-a.html" rel="alternate"></link><updated>2014-08-15T00:00:00+08:00</updated><author><name>Zeng Wenyuan</name></author><id>tag:zwy1135.github.io,2014-08-15:wan-zhuan-kagglezhi-titanic2hao-da-yi-tuo-shu-ju-a.html</id><summary type="html">&lt;p&gt;接上回。&lt;/p&gt;
&lt;p&gt;上回我们界定了我们要处理的问题，接下来就应该准备数据了。&lt;/p&gt;
&lt;h3&gt;什么数据才好？&lt;/h3&gt;
&lt;p&gt;准备数据的第一个问题，我们的数据从那来？有朋友一定要说了：这不明摆着的嘛，既然是kaggle上的问题，直接在kaggle上载下来不就完了嘛。对，这实在Kaggle上，但在实际应用中呢？实际应用中的数据可都是要自己去准备的。这是就要思考一个重要的事情，什么样的数据才是有用的呢？&lt;/p&gt;
&lt;p&gt;举个栗子，某天你的老板拎着一个U盘走过来对你说：“XX啊，我这有一份全市每天每一个菜市场里猪肉的价格表，你帮我看看明天哪支股票会涨。”这时你不禁脱口而出：“你TM在逗我？”然后你就被踢出去了，公司里再没人见过你。&lt;/p&gt;
&lt;p&gt;相信明眼人一定看出我想说什么了。对，跟&lt;strong&gt;&lt;em&gt;问题有关&lt;/em&gt;&lt;/strong&gt;的数据才是好数据，其他的一点价值都没有。&lt;/p&gt;
&lt;p&gt;在当前的问题下，我们要预测某人在沉船事故中能否幸存，我们就应该使用沉船事故中统计出来的数据。如果有人给你一份空难数据，你就可以直接糊他一脸了。好在Kaggle给我们准备好了数据，我们直接用就好。&lt;/p&gt;
&lt;h3&gt;第一件事&lt;/h3&gt;
&lt;p&gt;现在我们拿到数据了，让我们先看看里面有什么内容，万一能够看出点什么规律，我们就不用辛苦的做机器学习系统了。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;错！！！&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我们要坚信一条：数据和计算结果要比人用直觉去看出来的规律可靠得多的多的多！&lt;/p&gt;
&lt;p&gt;在这个认识下，我们的数据终究会进入机器进行分析的。而机器主要要做两件事情：1、训练模型 2、测试有效性 。怎么样才能正确测试有效性？用你完全无知的数据去测试有效性。任何对于测试数据的预先知识都会导致你设计系统的时候产生偏差。&lt;/p&gt;
&lt;p&gt;所以，我们要做的第一件事就是&lt;strong&gt;&lt;em&gt;拆分数据，把数据拆成训练集(Training Set)与测试集(Testing Set)&lt;/em&gt;&lt;/strong&gt;，之后就把测试集封存起来，除了数据结构之外，什么都不要了解。&lt;/p&gt;
&lt;p&gt;记住，系统定型前对测试集的每一分了解都会削弱系统对于未知数据的可用性。&lt;/p&gt;
&lt;p&gt;我们要怎样拆分数据呢？&lt;/p&gt;
&lt;p&gt;对于没有顺序要求的数据，随机抽取一部分数据就好。对于股票价格等有顺序的数据，按时间切片抽取。总之具体问题具体分析了。&lt;/p&gt;
&lt;p&gt;我们按什么比例拆分数据呢？&lt;/p&gt;
&lt;p&gt;一般情况下，训练及与测试集六四开。在数据量特别小时按八二开。总之要保证训练数据，大量的训练数据往往能比精巧的模型达到更好地效果。(放肆！你竟敢问只有一条数据怎么拆？来人，把这个捣乱的扔出去，交450都不要保他)&lt;/p&gt;
&lt;p&gt;现在，Kaggle帮我们把数据分拆都做完了我们也是直接用就行了。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;拿到了数据，我们就要把它处理成系统可接受的形式了。&lt;/p&gt;
&lt;h3&gt;导入数据&lt;/h3&gt;
&lt;p&gt;让我们看看，幸好我们拿到的数据是csv格式的表格，要是现在在任的数据攻城湿么看到这么规整的数据一定做梦都会笑出声来。要是给一段自然文本啊之类的，那就要写好大一段程序来处理了。&lt;/p&gt;
&lt;p&gt;不过，虽然我们拿到的数据很规整，但还是字符串和数值混排的形式。要自己把它处理成scikit-learn库可处理的形式也是么费好大一番功夫的。这里我用了名为pandas的数据分析库来方便的读入数据。如果有哪位坚毅之士愿意写循环一行行的读数据，那也是可以的。&lt;/p&gt;
&lt;p&gt;废话不说，Talk is cheap, show you the code.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;loadData&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;u&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    读入数据&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;rawData&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataPath&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;rawData&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;清理无用数据并数值化&lt;/h3&gt;
&lt;p&gt;读入数据之后，就要开始构建数据集，然而不是所有数据都要被输入系统里。我们要先吧数据筛选处理一遍。&lt;/p&gt;
&lt;p&gt;看看我们手上的数据，稍微有点常识的人都知道，自己在乘客名单上的序号是救不了人的。同理手上的票号也一样。另外，就算你叫龙傲天也不可能从海底飘起来。还有，船舱门牌号表明了船舱的位置，很可能影响了生存率，但是他的数据量太小，不适合加入所有数据里处理。之后可以专门开一章处理有门牌的数据。&lt;/p&gt;
&lt;p&gt;此外，除了极少数定制的数据处理库之外，绝大多数机器学习库都只接受数值作为输入数据，我们这个系统的核心scikit-learn也不例外，所以我们要将数据里的字符串映射为可以处理的数值。&lt;/p&gt;
&lt;p&gt;照例上代码&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;sexDict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;female&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;male&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="n"&gt;embarkDict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;S&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;C&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Q&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;buildDataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;isTrainData&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;u&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    清理无用数据并数值化，构建适合处理的数据集&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;SexNum&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Sex&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sexDict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;EmbarkedNum&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Embarked&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;embarkDict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;dropList&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;PassengerId&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Name&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Sex&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Ticket&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Cabin&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Embarked&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;isTrainData&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;dropList&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Survived&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Survived&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dropList&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;仔细的同学可能看到了，我作数值化就是是直接把字符串编号了。这样已经可以处理了，但是这不是最好的方法，因为数值之间是有大小关系的，1一定在0和2中间，但类别(Categories)没有，硬说 Bill 在 Alice 和 Christina 之间一点意义都没有。所以更稳妥的办法是将类别数据再编码为多个变量，有兴趣的同学可以了解下，&lt;a href="http://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features"&gt;链接在此&lt;/a&gt;。&lt;/p&gt;
&lt;h3&gt;数据补全&lt;/h3&gt;
&lt;p&gt;构建好数据集之后我们再看，这尼玛数据不全啊，东缺一块西缺一块的，输进模型里那还不是分分钟崩给你看的节奏啊，赶紧补一补。&lt;/p&gt;
&lt;p&gt;怎么补呢？&lt;/p&gt;
&lt;p&gt;常用的方法就是补上那一列的平均数，中位数或者最常出现的数。另外，还要注意数据缺失本来就带着一定的信息这种情况，比如说不随便告诉人们自己情况的人比较警觉，容易活下来。这时也可以直接在缺失处补0或者-100之类的常数。我这里就简单补了个平均值。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;impute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;imputer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;u&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    补全数据&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;imputer&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;imputer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Imputer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;missing_values&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;NaN&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;strategy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;mean&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;imputer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;newData&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;imputer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;newData&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;imputer&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;顺便说一句，这补齐里面也是有大学问的，近年来各路人马都纷纷发明了各种奇葩的补齐方法，有兴趣的同学可以搜一搜了解下。&lt;/p&gt;
&lt;p&gt;到这里，其实我们的数据已经可以输进模型里进行训练了。不过再进行一下润色可能会取得更好地效果。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;我是可做可不做的分割线&lt;/p&gt;
&lt;hr /&gt;
&lt;h3&gt;数据规范化&lt;/h3&gt;
&lt;p&gt;既然我们的数据可以用于训练了，那我们还要做什么呢？不要太着急，我们来看一下，我们手上的数据变化的幅度是不同的，按照常识，我们总会本能的认为在零到一亿变化的变量要比0到0.1变化的变量重要，机器经常也会真么认为，这有时会带来不必要的误差。我们希望能平等的分析各个变量，要怎么办呢？&lt;/p&gt;
&lt;p&gt;作数据规范化。&lt;/p&gt;
&lt;p&gt;什么是数据规范化？简单来说就是把数据放缩到同一个尺度进行分析。&lt;/p&gt;
&lt;p&gt;我这里把所有的数据放缩成均值为0标准差为1的标准数据集进行处理。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;standardize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;scaler&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;u&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    数据规范化&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;scaler&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;scaler&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;StandardScaler&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;scaler&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;newData&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;scaler&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;newData&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;scaler&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;另外还有把数据放缩到0，1之间的方法，各位可以自行了解一下。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;可做可不做的分界线结束&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;大功告成，喜大普奔，让我们把所有东西组合在一起看一下。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pp&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="n"&gt;dataPath&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;./data&amp;#39;&lt;/span&gt;

&lt;span class="n"&gt;sexDict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;female&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;male&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="n"&gt;embarkDict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;S&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;C&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Q&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;loadData&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;u&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    读入数据&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;rawData&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataPath&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;rawData&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;buildDataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;isTrainData&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;u&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    清理无用数据并数值化，构建适合处理的数据集&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;SexNum&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Sex&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sexDict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;EmbarkedNum&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Embarked&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;embarkDict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;dropList&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;PassengerId&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Name&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Sex&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Ticket&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Cabin&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Embarked&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;isTrainData&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;dropList&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Survived&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Survived&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dropList&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;impute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;imputer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;u&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    补全数据&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;imputer&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;imputer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Imputer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;missing_values&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;NaN&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;strategy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;mean&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;imputer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;newData&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;imputer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;newData&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;imputer&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;standardize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;scaler&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;u&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    数据规范化&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;scaler&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;scaler&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;StandardScaler&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;scaler&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;newData&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;scaler&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;newData&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;scaler&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;raw&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;loadData&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;train.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;buildDataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;imputer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;impute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;scaler&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;standardize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


    &lt;span class="n"&gt;test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;loadData&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;test.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;tmp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;buildDataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;tmp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;impute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;imputer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;tmp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;standardize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;scaler&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;什么？你问我为什么要把 imputer 和 scaler 反复使用，恭喜你终于找到了本章最大的重点，&lt;strong&gt;&lt;em&gt;作用于训练集上的改变要完全一致，不打任何折扣的作用在所有数据上&lt;/em&gt;&lt;/strong&gt;，否则你训练出来的模型将不能适用于任何地方，在别的地方一无是处，你整个工作都白费了。&lt;/p&gt;
&lt;p&gt;啊，你说这么做太麻烦了，我也这么觉得的。索性 scikit-learn 提供了Pipeline功能，我们可以把读进数据后的所有操作打包成一个管线，这么做就方便多了。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.pipeline&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Pipeline&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;raw&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;loadData&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;train.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;buildDataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;loadData&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;test.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;tmp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;buildDataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;pipel&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Pipeline&lt;/span&gt;&lt;span class="p"&gt;([(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;impute&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;pp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Imputer&lt;/span&gt;&lt;span class="p"&gt;()),(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;standardize&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;pp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;StandardScaler&lt;/span&gt;&lt;span class="p"&gt;())])&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pipel&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pipel&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;好了，数据就准备到这里，我们进入下一步。特征提取。&lt;/p&gt;
&lt;p&gt;Written with &lt;a href="https://stackedit.io/"&gt;StackEdit&lt;/a&gt;.&lt;/p&gt;</summary><category term="Machine Learning"></category><category term="Scikit-Learn"></category><category term="python"></category></entry><entry><title>玩转Kaggle之Titanic(1):这货到底在问什么？</title><link href="https://zwy1135.github.io/wan-zhuan-kagglezhi-titanic1zhe-huo-dao-di-zai-wen-shi-yao.html" rel="alternate"></link><updated>2014-08-13T00:00:00+08:00</updated><author><name>Zeng Wenyuan</name></author><id>tag:zwy1135.github.io,2014-08-13:wan-zhuan-kagglezhi-titanic1zhe-huo-dao-di-zai-wen-shi-yao.html</id><summary type="html">&lt;p&gt;接上回。&lt;/p&gt;
&lt;p&gt;我们在上回扯了一大堆淡，终于把要做的题目定下来了，从这一篇开始就要上干货了，可喜可贺，喜大普奔。。。。。那么，我们究竟要做点什么呢？&lt;/p&gt;
&lt;p&gt;多家游戏界的大佬们啊用自己的一款款大作告诉我们，关卡开始前一掉要先来一段任务简报舒缓下心情。我们来看看这里面有什么内容。&lt;/p&gt;
&lt;p&gt;在一望无际的大海上，马上就要沉掉的不沉之舟泰坦尼克载着杰克和罗丝飘荡着缓缓地向着目的地驶去，突然，航路上一座冰山狠狠地撞了过来。。。。。。&lt;/p&gt;
&lt;p&gt;喂喂，快醒醒，我们不是在拍电影啊，看重点。&lt;/p&gt;
&lt;p&gt;在那次事故中，2224人里面有1502人当场就被GM封号，永远的离开了地球Online。去的人自然是去了，活下来的人更要想尽办法继续活下去。那么，问题出现了，&lt;strong&gt;问&lt;/strong&gt;：在沉船事故发生这种&lt;strong&gt;状况&lt;/strong&gt;下(是沉船状况下，不是空难状况下。此点的重要性会在下一篇讲到)，某个人是不是可能&lt;strong&gt;活下来&lt;/strong&gt;？&lt;/p&gt;
&lt;p&gt;Bingo！ 各位看官，看到这里要打起精神来了，我们被问的是这个人&lt;strong&gt;&lt;em&gt;*是否&lt;/em&gt;&lt;/strong&gt;*能活下来而不是这个人能活&lt;strong&gt;&lt;em&gt;多长&lt;/em&gt;&lt;/strong&gt;，也不是这个人对自己活下来这件事的满意&lt;strong&gt;&lt;em&gt;程度&lt;/em&gt;&lt;/strong&gt;是多少。重点出来了：我们要求得的结果是&lt;strong&gt;&lt;em&gt;是&lt;/em&gt;&lt;/strong&gt;或者&lt;strong&gt;&lt;em&gt;否&lt;/em&gt;&lt;/strong&gt;，换句话说，我们要求得的&lt;strong&gt;&lt;em&gt;结果是&lt;/em&gt;&lt;/strong&gt;一个&lt;strong&gt;&lt;em&gt;离散的&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;类别(Categories)&lt;/em&gt;&lt;/strong&gt;，这就表明了我们现在要做的要么是聚类，要么就是分类了。&lt;/p&gt;
&lt;p&gt;再看看我们现在有什么。我们现在有一个登记表，上面记录着乘客票价，仓位等信息以及他们是否幸存。也就是说，我们&lt;strong&gt;&lt;em&gt;现有&lt;/em&gt;&lt;/strong&gt;一份&lt;strong&gt;&lt;em&gt;标记好&lt;/em&gt;&lt;/strong&gt;乘客是否生存的数据。&lt;/p&gt;
&lt;p&gt;现有有标记数据，求离散类别的结果，这就是一个妥妥的分类问题没跑了。&lt;/p&gt;
&lt;p&gt;谨记这一点，我们进入&lt;strong&gt;数据准备&lt;/strong&gt;阶段。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;我是愉快的分割线&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;上面闲扯了那么多有的没的东西，现在我来告诉大家一个简单的判断问题的方法：&lt;/p&gt;
&lt;p&gt;根据现有数据和所求的结果来判断：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;现有有标记数据，求离散的类别(Categories)结果   -&amp;gt; 分类(Classification)问题&lt;/p&gt;
&lt;p&gt;现有有标记数据，求连续的数值(Numerical)结果    -&amp;gt; 回归(Regression)问题&lt;/p&gt;
&lt;p&gt;现有无标记数据，求离散的类别结果               -&amp;gt; 聚类(Clustering)问题&lt;/p&gt;
&lt;p&gt;现有无标记数据，求连续的数值结果               -&amp;gt; 我也不知道什么问题&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;此外还有根据现象发生的时间求现象间的因果关系之类的问题等等其他问题，因为博主现在也不太熟悉。这里也不作细说。&lt;/p&gt;
&lt;p&gt;Written with &lt;a href="https://stackedit.io/"&gt;StackEdit&lt;/a&gt;.&lt;/p&gt;</summary><category term="Machine Learning"></category><category term="Scikit-Learn"></category><category term="python"></category></entry><entry><title>玩转Kaggle之Titanic(0):开坑宣言</title><link href="https://zwy1135.github.io/wan-zhuan-kagglezhi-titanic0kai-keng-xuan-yan.html" rel="alternate"></link><updated>2014-08-12T00:00:00+08:00</updated><author><name>Zeng Wenyuan</name></author><id>tag:zwy1135.github.io,2014-08-12:wan-zhuan-kagglezhi-titanic0kai-keng-xuan-yan.html</id><summary type="html">&lt;p&gt;观众朋友们，欢迎收看最新一期的Kaggle挑战赛娱乐实况解说。由于博主也是现学现卖，在本系列内容中，你们将不可避免的看到：博主犯二，代码错漏，方法行不通，前面全部推到重来等内容。遇到这种情况，其立即保护好自己的双眼并果断右上方点×。不过，如果有真的猛士能忍下这些糟糕的内容指点我一下的话，请在下方留言，谢谢。&lt;/p&gt;
&lt;p&gt;废话不多说，我们开始吧。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;先进一段背景。&lt;/p&gt;
&lt;h3&gt;什么是Kaggle？&lt;/h3&gt;
&lt;p&gt;Kaggle是由Predictive modelling公司于2010年四月推出的众包数据分析与竞赛平台。企业可以在上面以竞赛的形式发布自己数据分析的问题，寻求解决方法，而全世界任何人都可以注册，参与其中，提出自己的想法和解决方案，平台实时给予反馈结果，可以在排行榜中看到自己的位置。(引自&lt;a href="http://blog.sina.com.cn/s/blog_648ca2350101ho9p.html"&gt;这里&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;经过这么多年的发展，Kaggle在机器学习与数据分析领域中极具影响力，他的地位就像吉米多维奇习题集之于数学分析或者POJ之于ACM竞赛。&lt;/p&gt;
&lt;p&gt;总之对于对机器学习和数据科学感兴趣的同学，有事没事上去刷一刷题，勾搭一下各路大牛还是能起到活泼身心治疗失眠的作用的。&lt;/p&gt;
&lt;h3&gt;什么是Scikit-Learn？&lt;/h3&gt;
&lt;p&gt;Scikit-Learn是这一系列文章中主要用到的工具。它是Python下的一个重要的机器学习库，它实现了大部分经典的机器学习算法而且提供了方便使用又风格统一的接口。近年来使用人数不断上升，大有成为行业标准的趋势。更重要的是这是博主唯一会用的机器学习库，所以只能用它来卖弄一下了。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;了解完背景之后我们就可以进正题了。&lt;/p&gt;
&lt;p&gt;点开Kaggle的competitions页面，上面还是有很多正在进行的比赛的。不过，对于我们这种刚入门的新手肯定是不能上来就挑战那些难得吓人大神专用关卡。我们还是来看看101新手村里面有什么教学关卡好玩：&lt;/p&gt;
&lt;p&gt;手写数字识别、脸部关键点提取。。。图像识别啊，单是从图像里提取特征就够写一整系列的东西了，我们这些刚入门的渣渣就不要选这么高难度的东西了。&lt;/p&gt;
&lt;p&gt;Julia入门，Scikit-Learn官方教程。。。。这种连背景都没有就啪一段数据上来的东西，还让不让人愉快的玩耍了。&lt;/p&gt;
&lt;p&gt;泰坦尼克号上哪些人会是幸存者？这个问题好玩，就决定是你了皮卡，呃不，Titanic！。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;问题选定了，我们就可以进入机器学习问题的标准流程：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;考察问题 &amp;gt; 准备数据 &amp;gt; 提取特征 &amp;gt; 选定模型 &amp;gt; 确定参数 &amp;gt; 实施并评估效果&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;可以看出，上面这六个步骤环环相扣，每一步都构成了后面步骤的基础，一步都马虎不得。&lt;/p&gt;
&lt;p&gt;在接下来的一系列文章里，我会带着大家一步步走完这整个流程。敬请期待。&lt;/p&gt;
&lt;p&gt;Written with &lt;a href="https://stackedit.io/"&gt;StackEdit&lt;/a&gt;.&lt;/p&gt;</summary><category term="Machine Learning"></category><category term="Scikit-Learn"></category><category term="python"></category></entry><entry><title>Hello World！</title><link href="https://zwy1135.github.io/hello-world.html" rel="alternate"></link><updated>2014-08-11T00:00:00+08:00</updated><author><name>Zeng Wenyuan</name></author><id>tag:zwy1135.github.io,2014-08-11:hello-world.html</id><summary type="html">&lt;p&gt;断断续续折腾了好几天，终于是吧这个博客弄起来了。&lt;/p&gt;
&lt;p&gt;用pelican配的github pages，不得不说python2在windows下的编码问题真是令人蛋碎啊。从安装开始就不停的报unicodedecodeError。差点把我搞疯了。这是逼我转投python3的节奏么。&lt;/p&gt;
&lt;p&gt;不管怎样，问题终究是解决了。解法是在每一个报错的文件里加上&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sys&lt;/span&gt;
&lt;span class="nb"&gt;reload&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setdefaultencoding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getfilesystemencoding&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;不小心误入这里的朋友也许可以用上。&lt;/p&gt;
&lt;p&gt;最后感谢一下frantic1048提供的pelican使用&lt;a href="http://frantic1048.com/bo-ke-dan-sheng-ji-ji-yu-githubpelicanchuang-jian-bo-ke-de-zheng-ge-guo-cheng.html"&gt;教程&lt;/a&gt;。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Written with &lt;a href="https://stackedit.io/"&gt;StackEdit&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;</summary><category term="杂物"></category><category term="随笔"></category><category term="python"></category></entry></feed>