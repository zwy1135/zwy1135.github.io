<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>观测站</title><link href="https://zwy1135.github.io/" rel="alternate"></link><link href="https://zwy1135.github.io/feeds%5Czeng-wenyuan.atom.xml" rel="self"></link><id>https://zwy1135.github.io/</id><updated>2014-09-06T00:00:00+08:00</updated><entry><title>玩转Kaggle番外篇(0):没毛线你说个图啊</title><link href="https://zwy1135.github.io/wan-zhuan-kagglefan-wai-pian-0mei-mao-xian-ni-shuo-ge-tu-a.html" rel="alternate"></link><updated>2014-09-06T00:00:00+08:00</updated><author><name>Zeng Wenyuan</name></author><id>tag:zwy1135.github.io,2014-09-06:wan-zhuan-kagglefan-wai-pian-0mei-mao-xian-ni-shuo-ge-tu-a.html</id><summary type="html">&lt;p&gt;没错，不要怀疑你的眼睛，玩转kaggle系列出番外了。&lt;/p&gt;
&lt;p&gt;一个完整机器学习流程所要用到的东西，在本篇基本上讲了一遍。特地开个番外到底要讲什么呢？这还要问么，题目早已出卖了博主。没错就是要讲怎么画图。&lt;/p&gt;
&lt;p&gt;说到图片，这可是个好东西啊。俗话说一图胜千言，图片的信息容量和视觉表现力可不是干巴巴的文字能比得上的。不管是忽悠老师，对付上司还是坑害投资人，只要你需要向别人讲解什么的地方，你都需要一门特别的作图技巧。想像一下，假如有人正在向你推销他的成果，但是讲啦大半个钟头，你连一张图都没见到，你会是什么反应。对，没图你讲个毛线，自觉出门右转。&lt;/p&gt;
&lt;p&gt;言归正传，我们来讲作图。既然要做图，那就要有作图的工具吧。按照本系列一贯的画风，我们还是选用python作为我们的的编程语言。说道python的作图工具，那首当其冲的就是大名鼎鼎的matplotlib了。matplotlib作为最著名的python做图库，简单易用，功能完备，最重要的是其作图函数和matlab近似。这么好用的东西不能不给大家推荐下。&lt;/p&gt;
&lt;p&gt;但是，博主我既然专门开了一篇博客来讲作图，自然不能讲这个百度一搜一大片的大路货。再说了，这么传统的东西怎么能满足博主作为一个追新党的心情。要讲，那自然是将最新最潮的东西。现在最酷炫的做图库是什么呢？当然是最近风头正劲的D3.js了。不过不对啊，这货是javascript的库，和我们画风不一样啊。哈，说出这样的话就太小看作为著名胶水语言的python了。就算是神，我们也能给他套上一层python的外壳。而D3.js套上python壳后，就华丽丽的变身成我们这一章的主角——Bokeh。&lt;/p&gt;
&lt;p&gt;说这么多，上图啊混蛋。好啦好啦，不要心急，这就上干货。我们还是拿Titanic的数据来举例。关于数据读入之类的代码可以在正篇(2)找到。&lt;/p&gt;
&lt;h3&gt;直方图/条形图&lt;/h3&gt;
&lt;p&gt;说起作图，那最基本的肯定是点(散点)线(曲线、折线)条(条形)方(直方)四类了。而直方图呢，又可以归类到条形图里。可以说条形图/直方图视作常用的图表之二了。我们就先讲这两样吧。&lt;/p&gt;
&lt;p&gt;先上代码：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;loadData&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;train.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;buildDataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;data_to_plot&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Age&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;data_to_plot&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data_to_plot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropna&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;edges&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;histogram&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_to_plot&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;bins&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output_file&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;histogram.html&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hold&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;quad&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;top&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;bottom&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;left&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;edges&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;right&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;edges&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:],&lt;/span&gt;
        &lt;span class="n"&gt;fill_color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;#036564&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;line_color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;#033649&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;bokeh不像matplotlib，它没有histogram函数，所以我们需要借助numpy的力量。其次呢，bokeh不能容忍NaN数据。一旦提供的数据不全就无法绘制图像。各位在使用时要注意了。&lt;/p&gt;
&lt;p&gt;得到的结果如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt="histogram" src="/image/histogram.png" /&gt;&lt;/p&gt;
&lt;p&gt;从上图我们可以看出，Titanic上最常出现的还是二十到三十岁之间的年轻人。另外还有不少孩子，想来是一家三口居家旅行的节奏了。可惜这一去就没多少人回来。&lt;/p&gt;
&lt;h3&gt;散点图&lt;/h3&gt;
&lt;p&gt;直方图虽然能直观的显示数据的分布情况，但他的弱点就在于一次只能显示一个特征的分布情况。视觉展示力还是略逊一筹。要直观的展示多个特征的联合分布情况。那就要请出点线条方之首，散点图了。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;loadData&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;train.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;buildDataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;data_to_plot&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropna&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output_file&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;scatter.html&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hold&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Fare&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Age&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
           &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Survived&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;colormap&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
           &lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Fare&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;其分布如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt="scatter" src="/image/scatter.png" /&gt;&lt;/p&gt;
&lt;p&gt;单从这个分布来看，按年龄和票价来分生死还是不容易的。不过买的起500多的票的土豪们还是无一死亡倒是事实。&lt;/p&gt;
&lt;h3&gt;线图&lt;/h3&gt;
&lt;p&gt;线图的最长项是表达数据的变化趋势。我们这回用的数据并没有时间上的关系，这个长处就表达不出来了，我们还是来画画直方图的分布轮廓线吧。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="s-Atom"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;loadData&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;train.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="s-Atom"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s-Atom"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;buildDataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nv"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="s-Atom"&gt;data_to_plot&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s-Atom"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Age&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="s-Atom"&gt;data_to_plot&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s-Atom"&gt;data_to_plot&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="s-Atom"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;isnan&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;data_to_plot&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

&lt;span class="s-Atom"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s-Atom"&gt;edges&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s-Atom"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;histogram&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;data_to_plot&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s-Atom"&gt;bins&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="s-Atom"&gt;pl&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;output_file&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;&amp;#39;histogram.html&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="s-Atom"&gt;pl&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="s-Atom"&gt;pl&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;hold&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="s-Atom"&gt;pl&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;quad&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;top&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s-Atom"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s-Atom"&gt;bottom&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s-Atom"&gt;left&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s-Atom"&gt;edges&lt;/span&gt;&lt;span class="p"&gt;[:-&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="s-Atom"&gt;right&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s-Atom"&gt;edges&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="s-Atom"&gt;:&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="s-Atom"&gt;fill_color=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;#036564&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s-Atom"&gt;line_color=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;#033649&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s-Atom"&gt;title=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Age distribution&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s-Atom"&gt;xlabel=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Age&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s-Atom"&gt;ylabel=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Number&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="s-Atom"&gt;pl&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;line&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s-Atom"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;average&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s-Atom"&gt;edges&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="s-Atom"&gt;:&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="s-Atom"&gt;edges&lt;/span&gt;&lt;span class="p"&gt;[:-&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="s-Atom"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="s-Atom"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s-Atom"&gt;line_width&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s-Atom"&gt;color=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;red&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s-Atom"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="m"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;效果嘛，就象这样：&lt;/p&gt;
&lt;p&gt;&lt;img alt="line" src="/image/line.png" /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;bokeh的能耐远远不止这点基本功能，有兴趣的朋友可以了解下他们的&lt;a href="http://bokeh.pydata.org/index.html"&gt;官方文档&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;这回就到这吧。各位，下回见。&lt;/p&gt;
&lt;p&gt;Written with &lt;a href="https://stackedit.io/"&gt;StackEdit&lt;/a&gt;.&lt;/p&gt;</summary><category term="Machine Learning"></category><category term="Bokeh"></category><category term="python"></category></entry><entry><title>笔试有感</title><link href="https://zwy1135.github.io/bi-shi-you-gan.html" rel="alternate"></link><updated>2014-08-29T00:00:00+08:00</updated><author><name>Zeng Wenyuan</name></author><id>tag:zwy1135.github.io,2014-08-29:bi-shi-you-gan.html</id><summary type="html">&lt;p&gt;今天去阿里的笔试。&lt;/p&gt;
&lt;p&gt;笔试分两部分，一部分选择，一部分编程。我做完选择之后，顺手点了提交。结果告诉我两部分是一起提交的。结果八十分钟的编程时间都浪费在客服那恶心的一分钟直接断线系统上了。&lt;/p&gt;
&lt;p&gt;这让我说什么好。。。。。。。这时候只要微笑就好了吧。&lt;/p&gt;
&lt;p&gt;算了，抱怨也没什么意思。还是继续刷刷LeetCode，准备下一家吧。&lt;/p&gt;
&lt;p&gt;Written with &lt;a href="https://stackedit.io/"&gt;StackEdit&lt;/a&gt;.&lt;/p&gt;</summary><category term="ali"></category><category term="Written Test"></category></entry><entry><title>玩转Kaggle之Titanic(5):参数调优——燃烧吧CPU</title><link href="https://zwy1135.github.io/wan-zhuan-kagglezhi-titanic5can-shu-diao-you-ran-shao-ba-cpu.html" rel="alternate"></link><updated>2014-08-27T00:00:00+08:00</updated><author><name>Zeng Wenyuan</name></author><id>tag:zwy1135.github.io,2014-08-27:wan-zhuan-kagglezhi-titanic5can-shu-diao-you-ran-shao-ba-cpu.html</id><summary type="html">&lt;p&gt;接上回。&lt;/p&gt;
&lt;p&gt;上回我们确定了模型。也用默认参数偷跑了一下测试集，效果一般般。所以，我们这会就来讲讲怎么才能把参数调到最好吧。&lt;/p&gt;
&lt;p&gt;既然要参数调优，我们就需要一个评判优劣的标准吧。要用什么所标准才好？当然是结果了。正所谓效果好才是真的好，我们所有的努力都是为了追求一个好的分类结果。那么，用结果作为评判标准就是顺理成章的事情了。&lt;/p&gt;
&lt;p&gt;呃，好像漏了点什么东西。我们用什么的结果作为评判标准啊？既然我们要测试效果，那么就用测试集的分类正确率好了。错！！你忘了我前面反复讲的东西了么？在系统定型之前不要碰测试集！赶紧给我滚回去用训练集去。&lt;/p&gt;
&lt;p&gt;好吧，我们来用训练集。但是训练集都用来训练了，再用它测试，结果能准么？肯定不准。那么怎么把训练集用起来呢？这就要讲到本章第一个方法了。&lt;/p&gt;
&lt;hr /&gt;
&lt;h3&gt;交叉验证(Cross Validation)&lt;/h3&gt;
&lt;p&gt;既然要做效果测试那么就不能用相同的数据既做训练又做测试了，但我们只有一个训练集可以用啊，怎么办呢？一个字：拆。把训练及再做拆分，拆出一个训练集和测试集出来，用新的训练集做训练，新的测试集做测试，这样就避免了同一组数据既做训练又做测试的问题了。但这样出来的结果能准吗？肯定不准。那怎么办？还是那个字：拆，而且要反复拆。每次换用不同的拆分方法把训练集拆分成新的训练集和测试集。在用这一堆新训练集和测试集对同一个分类器进行训练和测试。把测试的结果汇总起来，我们就可以估计出这个分类器在实际使用中的效果了。(当然，这是在假定训练集反映了实际运用场景中数据的规律的情况，要是使用的训练集根本和实际数据毫无关系的话，我们做什么都白搭。)&lt;/p&gt;
&lt;p&gt;以上就是交叉验证的意义了。&lt;/p&gt;
&lt;p&gt;具体怎么做，放码上来。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.cross_validation&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ShuffleSplit&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;cross_val_score&lt;/span&gt;
&lt;span class="n"&gt;classifier&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LinearSVC&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="c"&gt;#我们的数据没有什么顺序相关性，所以这里就用随机抽取的方法拆出10组训练集和测试集&lt;/span&gt;
&lt;span class="n"&gt;cv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ShuffleSplit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cross_val_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;cv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;cv&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Accuracy is &lt;/span&gt;&lt;span class="si"&gt;%.2f&lt;/span&gt;&lt;span class="s"&gt; (+/- &lt;/span&gt;&lt;span class="si"&gt;%.2f&lt;/span&gt;&lt;span class="s"&gt;)&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;&lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;得到的结果是0.80 (+/- 0.07)符合我们上次得到的结果了。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;有了评判标准之后，我们就可以做参数调优了。到底怎么调？有没有什么简单易懂的现代方法好啊确定最优参数啊。&lt;/p&gt;
&lt;p&gt;还真没有。现在的参数确定和星座算命一样，说多了都是玄学啊。那怎么办？&lt;/p&gt;
&lt;p&gt;别急，神说了：给我无限的运算力，我能用穷举法解决一切问题。虽然我们没有无限的运算力，可还是可以试一试穷举的。&lt;/p&gt;
&lt;p&gt;那还等什么，赶紧的一个个参数试啊。&lt;/p&gt;
&lt;p&gt;慢着，现在是信息时代了，一切都要自动化。人生苦短，可不能把时间浪费在设参数这种没意义的事情上，这就要请出本章的第二种方法了。&lt;/p&gt;
&lt;hr /&gt;
&lt;h3&gt;网格搜索(Grid Search)&lt;/h3&gt;
&lt;p&gt;网格搜索，作为最基础的参数确定方法。在每一个机器学习库中都会提供。其主要功能就是根据用户提供的所有可能的参数，自动排列出所有的组合，然后一个个应用到模型中计算出效果最好的参数组合。&lt;/p&gt;
&lt;p&gt;使用方法，惯例打码。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.grid_search&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GridSearchCV&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;GSC&lt;/span&gt;
&lt;span class="c"&gt;#因为线性SVC也可以归在SVC里面，所以就放在SVC里一起算了&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.svm&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SVC&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.cross_validation&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ShuffleSplit&lt;/span&gt;

&lt;span class="n"&gt;svc_parameter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;C&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
             &lt;span class="s"&gt;&amp;#39;kernel&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;linear&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;rbf&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;sigmoid&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
             &lt;span class="s"&gt;&amp;#39;degree&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
             &lt;span class="s"&gt;&amp;#39;gamma&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
             &lt;span class="s"&gt;&amp;#39;coef0&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
             &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="c"&gt;#这里一定要设置max_iter，不然万一某个模型不收敛那就要算到天荒地老了&lt;/span&gt;
&lt;span class="n"&gt;classifier&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SVC&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_iter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1e6&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ShuffleSplit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;gsc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GSC&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;tree_parameter&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;cv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;cv&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;fitting data&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;gsc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;fitdone&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;bestClassifier&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gsc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_estimator_&lt;/span&gt;
&lt;span class="n"&gt;score&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gsc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_score_&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;best&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;启动程序，现在你可以从电脑前离开，去喝个咖啡看场电影，洗个澡，睡一觉。等你醒来的时候，你就会得到。。。。。。。。。。。。。。。。一个算烧掉的CPU。&lt;/p&gt;
&lt;p&gt;开玩笑的，不过这段程序运算量很大是真的。博主我i5的CPU算了超过六个小时，各位可以看情况估计下自己的等待时间。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;好了，到这里我们是终于把我们的整个分类系统完成了。真是激动人心的时刻啊。&lt;/p&gt;
&lt;p&gt;好，我们终于要正式对测试集下手了。predict。。。save。。。上传到Kaggle。。。最终结果。。。正确率77.99%。刚好卡在benchmark的位置。要想再往上提就要有特别的机器学习技巧了。这里按下不表。&lt;/p&gt;
&lt;p&gt;所有功能统合在一起的程序，等我改天整理好之后再发一篇总结吧。这一系列文章的主要内容就差不多到这里了。&lt;/p&gt;
&lt;p&gt;感谢能耐着性子看到这里的朋友。如果大家发现了什么错漏的地方，希望能不吝赐教。&lt;/p&gt;
&lt;p&gt;谢谢各位。让我们下一个系列再见。&lt;/p&gt;
&lt;p&gt;Written with &lt;a href="https://stackedit.io/"&gt;StackEdit&lt;/a&gt;.&lt;/p&gt;</summary><category term="Machine Learning"></category><category term="Scikit-Learn"></category><category term="python"></category></entry><entry><title>玩转Kaggle之Titanic(4):按图索器</title><link href="https://zwy1135.github.io/wan-zhuan-kagglezhi-titanic4an-tu-suo-qi.html" rel="alternate"></link><updated>2014-08-22T00:00:00+08:00</updated><author><name>Zeng Wenyuan</name></author><id>tag:zwy1135.github.io,2014-08-22:wan-zhuan-kagglezhi-titanic4an-tu-suo-qi.html</id><summary type="html">&lt;p&gt;接上回。&lt;/p&gt;
&lt;p&gt;上回我们筛选了特征。现在就该把数据输进分类器里开始训练分类起了。&lt;/p&gt;
&lt;p&gt;慢着，我们好像漏了什么关键的东西。。。。。。。对了，我们还没决定用哪个分类器呐，到底用那种分类器才好？&lt;/p&gt;
&lt;p&gt;这可是个见功底的大问题啊，这分类器万一选不好，训练时间加长还不是问题，万一分类的准确程度达不到要求，我们辛辛苦苦处理出来数据的努力就全泡汤了。到底我们该怎么选分类器啊？有没有什么猴子都能看懂的秘籍，让我们这种什么经验都没有的菜鸟也能方便的学会怎么选分类器啊。&lt;/p&gt;
&lt;p&gt;嘿，还真让我找到了这么个东西。我把它称之为究极无敌超超超方便策略选取图。有了这幅图，妈妈再也不用担心我们碰到问题没有头绪了。&lt;/p&gt;
&lt;p&gt;废话少说，一图胜千言。&lt;/p&gt;
&lt;p&gt;&lt;img alt="ml_map" src="/image/ml_map.png" /&gt;&lt;/p&gt;
&lt;p&gt;(大家看了就知道，这图是我从scikit-learn官网文档上，弄下来的。连接在&lt;a href="http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html"&gt;这里&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;有图在手，我们可以选分类器了。让我来看看，我们有类别(Categories)数据，有Label，数据量在100K以下。就决定是你了LinearSVC。&lt;/p&gt;
&lt;p&gt;好，让我们把数据输进去吧。(其实不应该这时候输进去的。记得我之前说过什么吗？保持对测试集的无知，直到我们的系统大致定型。不过我们又不是实战，就偷偷满足下自己的好奇心好了。)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;classifier&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LinearSVC&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;正确率74.641%。。。。真心不高啊。这是最好的结果了么？天知道(肯定不是啊喂，kaggle上有人刷到正确率100%啊)。不过没关系，我们还有好多东西没做那。做完正确率会更高的。&lt;/p&gt;
&lt;p&gt;Written with &lt;a href="https://stackedit.io/"&gt;StackEdit&lt;/a&gt;.&lt;/p&gt;</summary><category term="Machine Learning"></category><category term="Scikit-Learn"></category><category term="python"></category></entry><entry><title>玩转Kaggle之Titanic(3):该用哪个特征啊？</title><link href="https://zwy1135.github.io/wan-zhuan-kagglezhi-titanic3gai-yong-na-ge-te-zheng-a.html" rel="alternate"></link><updated>2014-08-20T00:00:00+08:00</updated><author><name>Zeng Wenyuan</name></author><id>tag:zwy1135.github.io,2014-08-20:wan-zhuan-kagglezhi-titanic3gai-yong-na-ge-te-zheng-a.html</id><summary type="html">&lt;p&gt;接上回。&lt;/p&gt;
&lt;p&gt;上回说到我们准备好了数据，可以进行特征提取了。&lt;/p&gt;
&lt;p&gt;呃，其实这里是我用词不当，特征提取(Feature extraction)是从不规则数据中提取出特征，转化为我们的分类期可以用的形式。比如说将自然文本提取词频啦，词语的前后关系啦……做成类似我们使用的这种表格的形式。这也是可以再开一个系列讲的东西。这里也不赘述了。&lt;/p&gt;
&lt;p&gt;这一篇我想讲的其实是特征选取(Feature selection)。换句话说就是计算那些东西该输进分类器里，哪些该扔掉。&lt;/p&gt;
&lt;p&gt;为什么要选取特征？把所有的数据都输进分类器里让他自己训练不好么？&lt;/p&gt;
&lt;p&gt;还真不好。&lt;/p&gt;
&lt;p&gt;因为，数据维度加大也就是数据列越多会导致分类期的训练时间飞速上涨。&lt;/p&gt;
&lt;p&gt;我们这回用的数据就那么几列，怎样都无所谓了。但是给你一段淘宝啊亚马逊啊的交易数据或者人类遗传编码数据呢，那可是轻轻松松破百万列的数据啊。要是都扔进分类器里应算，那你就做好准备算到天荒地老吧。(去超算中心一包就包年的土豪们，请自动无视这段话。)&lt;/p&gt;
&lt;p&gt;另外，将多余数据加入到分类器也会增加过度拟合(over-fitting)的风险。&lt;/p&gt;
&lt;p&gt;什么是过度拟合呢？简单来说就是分类器计算出来的规则太过于契合训练集导致它在训练集外的效果下降。再举个例子，假如现在有一个判断性别的任务，给的数据有Y染色体数目，发色，头发长度，肤色，指甲长度。正确情况下，分类器应该只根据Y染色体数目判断性别。但一旦分类器过拟合了，它就可能拟合出Y染色体数目不为零，头发长度小于10 CM，发色与肤色都不为白色者为男性这种规则出来。&lt;/p&gt;
&lt;p&gt;说了这么多，到底我们应该怎么做特征提取啊？&lt;/p&gt;
&lt;p&gt;两个思路(其实都差不多了):掐头，去尾。&lt;/p&gt;
&lt;h4&gt;掐头&lt;/h4&gt;
&lt;p&gt;所谓掐头就是说提取出重要程度最高的特征。&lt;/p&gt;
&lt;p&gt;怎么确认特征的重要性呢，那就要看各位的统计功底了，你可以把各特征与标签分别作卡方检验、F检验、p值检验等。甚至是拿一部分训练数据直接丢进分类器里训练，然后检查分类器对各特征的重要性评估数据。&lt;/p&gt;
&lt;p&gt;对于这种思路Scikit-learn 提供了&lt;a href="http://scikit-learn.org/stable/modules/feature_selection.html"&gt;好几种方法&lt;/a&gt;。比如说选取最好的k个特征&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest"&gt;SelectKBest&lt;/a&gt;，选取最好的百分之p的特征&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html#sklearn.feature_selection.SelectPercentile"&gt;SelectPercentile&lt;/a&gt;等等。&lt;/p&gt;
&lt;h4&gt;去尾&lt;/h4&gt;
&lt;p&gt;所谓去尾就是将不重要的特征砍掉。&lt;/p&gt;
&lt;p&gt;什么是不重要的特征呢？还拿上面判断性别的任务来说，假如现在你的数据里有一列是眼睛的数量。这个量在每一条数据里都为2，你认为这个量重不重要呢？换句话说，变化率在某一个值以下的特征是不重要的。(这个判断隐含了一个前提条件，即训练集中每一个分类的数量都是足量的，如果每一类都足量的前提下一个变量的变化率还是很低，那么它就应该和每一个类都没关系)&lt;/p&gt;
&lt;p&gt;在scikit-learn里面这个思路是由 VarianceThreshold 实现的。(居然这个函数不在API文档里的-_-!,大家到源码里面去看吧)&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;在这里，我们用的数据也就那么几列，掐完头就没什么数据了，所以我就只做了去尾的工作来排除无效数据。代码如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.feature_selection&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;VarianceThreshold&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;selection&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;selector&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;VarianceThreshold&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;threshold&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;selector&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;selector&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;hr /&gt;
&lt;p&gt;另外呢，从广义上来说，特征选取也是一个数据降维的过程。所以也可以用PCA这类的方法来做。有兴趣的同学可以看一看这个&lt;a href="scikit-learn.org/stable/modules/decomposition.html"&gt;链接&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Written with &lt;a href="https://stackedit.io/"&gt;StackEdit&lt;/a&gt;.&lt;/p&gt;</summary><category term="Machine Learning"></category><category term="Scikit-Learn"></category><category term="python"></category></entry><entry><title>转投python3了</title><link href="https://zwy1135.github.io/zhuan-tou-python3liao.html" rel="alternate"></link><updated>2014-08-19T00:00:00+08:00</updated><author><name>Zeng Wenyuan</name></author><id>tag:zwy1135.github.io,2014-08-19:zhuan-tou-python3liao.html</id><summary type="html">&lt;p&gt;这几天关注了一下 python3 的兼容性情况，发现我平时常用的包都早就兼容 python3 了。又想起前段时间折腾的要死的 python2 编码问题，总觉的是时候拥抱新技术了。于是呢这一篇博客就成了我用 python3 版的 pelican 编译的第一篇博客了，可喜可贺，特此留念。&lt;/p&gt;
&lt;p&gt;另外这几天发现了一个叫 conda 的 python 包管理器，甚是好用啊。最主要的是它用二进制安装包的形式来管理各种 python 包，而不是像 pip 一样管理源码包。这对于像我这种主要使用 windows 的家伙简直是天大的福音啊。从此更新 numpy 和 scipy 再也不用自己去下安装包了。再也不用折腾 lapack 和 blas 这种蛋疼得问题库了。&lt;/p&gt;
&lt;p&gt;Written with &lt;a href="https://stackedit.io/"&gt;StackEdit&lt;/a&gt;.&lt;/p&gt;</summary><category term="python"></category><category term="conda"></category></entry><entry><title>玩转Kaggle之Titanic(2):好大一坨数据啊</title><link href="https://zwy1135.github.io/wan-zhuan-kagglezhi-titanic2hao-da-yi-tuo-shu-ju-a.html" rel="alternate"></link><updated>2014-08-15T00:00:00+08:00</updated><author><name>Zeng Wenyuan</name></author><id>tag:zwy1135.github.io,2014-08-15:wan-zhuan-kagglezhi-titanic2hao-da-yi-tuo-shu-ju-a.html</id><summary type="html">&lt;p&gt;接上回。&lt;/p&gt;
&lt;p&gt;上回我们界定了我们要处理的问题，接下来就应该准备数据了。&lt;/p&gt;
&lt;h3&gt;什么数据才好？&lt;/h3&gt;
&lt;p&gt;准备数据的第一个问题，我们的数据从那来？有朋友一定要说了：这不明摆着的嘛，既然是kaggle上的问题，直接在kaggle上载下来不就完了嘛。对，这实在Kaggle上，但在实际应用中呢？实际应用中的数据可都是要自己去准备的。这是就要思考一个重要的事情，什么样的数据才是有用的呢？&lt;/p&gt;
&lt;p&gt;举个栗子，某天你的老板拎着一个U盘走过来对你说：“XX啊，我这有一份全市每天每一个菜市场里猪肉的价格表，你帮我看看明天哪支股票会涨。”这时你不禁脱口而出：“你TM在逗我？”然后你就被踢出去了，公司里再没人见过你。&lt;/p&gt;
&lt;p&gt;相信明眼人一定看出我想说什么了。对，跟&lt;strong&gt;&lt;em&gt;问题有关&lt;/em&gt;&lt;/strong&gt;的数据才是好数据，其他的一点价值都没有。&lt;/p&gt;
&lt;p&gt;在当前的问题下，我们要预测某人在沉船事故中能否幸存，我们就应该使用沉船事故中统计出来的数据。如果有人给你一份空难数据，你就可以直接糊他一脸了。好在Kaggle给我们准备好了数据，我们直接用就好。&lt;/p&gt;
&lt;h3&gt;第一件事&lt;/h3&gt;
&lt;p&gt;现在我们拿到数据了，让我们先看看里面有什么内容，万一能够看出点什么规律，我们就不用辛苦的做机器学习系统了。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;错！！！&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我们要坚信一条：数据和计算结果要比人用直觉去看出来的规律可靠得多的多的多！&lt;/p&gt;
&lt;p&gt;在这个认识下，我们的数据终究会进入机器进行分析的。而机器主要要做两件事情：1、训练模型 2、测试有效性 。怎么样才能正确测试有效性？用你完全无知的数据去测试有效性。任何对于测试数据的预先知识都会导致你设计系统的时候产生偏差。&lt;/p&gt;
&lt;p&gt;所以，我们要做的第一件事就是&lt;strong&gt;&lt;em&gt;拆分数据，把数据拆成训练集(Training Set)与测试集(Testing Set)&lt;/em&gt;&lt;/strong&gt;，之后就把测试集封存起来，除了数据结构之外，什么都不要了解。&lt;/p&gt;
&lt;p&gt;记住，系统定型前对测试集的每一分了解都会削弱系统对于未知数据的可用性。&lt;/p&gt;
&lt;p&gt;我们要怎样拆分数据呢？&lt;/p&gt;
&lt;p&gt;对于没有顺序要求的数据，随机抽取一部分数据就好。对于股票价格等有顺序的数据，按时间切片抽取。总之具体问题具体分析了。&lt;/p&gt;
&lt;p&gt;我们按什么比例拆分数据呢？&lt;/p&gt;
&lt;p&gt;一般情况下，训练及与测试集六四开。在数据量特别小时按八二开。总之要保证训练数据，大量的训练数据往往能比精巧的模型达到更好地效果。(放肆！你竟敢问只有一条数据怎么拆？来人，把这个捣乱的扔出去，交450都不要保他)&lt;/p&gt;
&lt;p&gt;现在，Kaggle帮我们把数据分拆都做完了我们也是直接用就行了。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;拿到了数据，我们就要把它处理成系统可接受的形式了。&lt;/p&gt;
&lt;h3&gt;导入数据&lt;/h3&gt;
&lt;p&gt;让我们看看，幸好我们拿到的数据是csv格式的表格，要是现在在任的数据攻城湿么看到这么规整的数据一定做梦都会笑出声来。要是给一段自然文本啊之类的，那就要写好大一段程序来处理了。&lt;/p&gt;
&lt;p&gt;不过，虽然我们拿到的数据很规整，但还是字符串和数值混排的形式。要自己把它处理成scikit-learn库可处理的形式也是么费好大一番功夫的。这里我用了名为pandas的数据分析库来方便的读入数据。如果有哪位坚毅之士愿意写循环一行行的读数据，那也是可以的。&lt;/p&gt;
&lt;p&gt;废话不说，Talk is cheap, show you the code.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;loadData&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;u&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    读入数据&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;rawData&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataPath&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;rawData&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;清理无用数据并数值化&lt;/h3&gt;
&lt;p&gt;读入数据之后，就要开始构建数据集，然而不是所有数据都要被输入系统里。我们要先吧数据筛选处理一遍。&lt;/p&gt;
&lt;p&gt;看看我们手上的数据，稍微有点常识的人都知道，自己在乘客名单上的序号是救不了人的。同理手上的票号也一样。另外，就算你叫龙傲天也不可能从海底飘起来。还有，船舱门牌号表明了船舱的位置，很可能影响了生存率，但是他的数据量太小，不适合加入所有数据里处理。之后可以专门开一章处理有门牌的数据。&lt;/p&gt;
&lt;p&gt;此外，除了极少数定制的数据处理库之外，绝大多数机器学习库都只接受数值作为输入数据，我们这个系统的核心scikit-learn也不例外，所以我们要将数据里的字符串映射为可以处理的数值。&lt;/p&gt;
&lt;p&gt;照例上代码&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;sexDict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;female&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;male&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="n"&gt;embarkDict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;S&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;C&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Q&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;buildDataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;isTrainData&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;u&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    清理无用数据并数值化，构建适合处理的数据集&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;SexNum&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Sex&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sexDict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;EmbarkedNum&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Embarked&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;embarkDict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;dropList&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;PassengerId&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Name&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Sex&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Ticket&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Cabin&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Embarked&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;isTrainData&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;dropList&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Survived&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Survived&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dropList&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;仔细的同学可能看到了，我作数值化就是是直接把字符串编号了。这样已经可以处理了，但是这不是最好的方法，因为数值之间是有大小关系的，1一定在0和2中间，但类别(Categories)没有，硬说 Bill 在 Alice 和 Christina 之间一点意义都没有。所以更稳妥的办法是将类别数据再编码为多个变量，有兴趣的同学可以了解下，&lt;a href="http://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features"&gt;链接在此&lt;/a&gt;。&lt;/p&gt;
&lt;h3&gt;数据补全&lt;/h3&gt;
&lt;p&gt;构建好数据集之后我们再看，这尼玛数据不全啊，东缺一块西缺一块的，输进模型里那还不是分分钟崩给你看的节奏啊，赶紧补一补。&lt;/p&gt;
&lt;p&gt;怎么补呢？&lt;/p&gt;
&lt;p&gt;常用的方法就是补上那一列的平均数，中位数或者最常出现的数。另外，还要注意数据缺失本来就带着一定的信息这种情况，比如说不随便告诉人们自己情况的人比较警觉，容易活下来。这时也可以直接在缺失处补0或者-100之类的常数。我这里就简单补了个平均值。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;impute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;imputer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;u&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    补全数据&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;imputer&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;imputer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Imputer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;missing_values&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;NaN&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;strategy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;mean&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;imputer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;newData&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;imputer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;newData&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;imputer&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;顺便说一句，这补齐里面也是有大学问的，近年来各路人马都纷纷发明了各种奇葩的补齐方法，有兴趣的同学可以搜一搜了解下。&lt;/p&gt;
&lt;p&gt;到这里，其实我们的数据已经可以输进模型里进行训练了。不过再进行一下润色可能会取得更好地效果。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;我是可做可不做的分割线&lt;/p&gt;
&lt;hr /&gt;
&lt;h3&gt;数据规范化&lt;/h3&gt;
&lt;p&gt;既然我们的数据可以用于训练了，那我们还要做什么呢？不要太着急，我们来看一下，我们手上的数据变化的幅度是不同的，按照常识，我们总会本能的认为在零到一亿变化的变量要比0到0.1变化的变量重要，机器经常也会真么认为，这有时会带来不必要的误差。我们希望能平等的分析各个变量，要怎么办呢？&lt;/p&gt;
&lt;p&gt;作数据规范化。&lt;/p&gt;
&lt;p&gt;什么是数据规范化？简单来说就是把数据放缩到同一个尺度进行分析。&lt;/p&gt;
&lt;p&gt;我这里把所有的数据放缩成均值为0标准差为1的标准数据集进行处理。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;standardize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;scaler&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;u&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    数据规范化&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;scaler&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;scaler&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;StandardScaler&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;scaler&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;newData&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;scaler&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;newData&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;scaler&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;另外还有把数据放缩到0，1之间的方法，各位可以自行了解一下。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;可做可不做的分界线结束&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;大功告成，喜大普奔，让我们把所有东西组合在一起看一下。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pp&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="n"&gt;dataPath&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;./data&amp;#39;&lt;/span&gt;

&lt;span class="n"&gt;sexDict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;female&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;male&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="n"&gt;embarkDict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;S&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;C&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Q&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;loadData&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;u&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    读入数据&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;rawData&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataPath&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;rawData&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;buildDataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;isTrainData&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;u&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    清理无用数据并数值化，构建适合处理的数据集&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;SexNum&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Sex&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sexDict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;EmbarkedNum&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Embarked&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;embarkDict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;dropList&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;PassengerId&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Name&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Sex&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Ticket&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Cabin&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Embarked&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;isTrainData&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;dropList&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Survived&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Survived&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dropList&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;impute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;imputer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;u&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    补全数据&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;imputer&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;imputer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Imputer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;missing_values&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;NaN&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;strategy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;mean&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;imputer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;newData&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;imputer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;newData&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;imputer&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;standardize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;scaler&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;u&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    数据规范化&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;scaler&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;scaler&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;StandardScaler&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;scaler&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;newData&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;scaler&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;newData&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;scaler&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;raw&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;loadData&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;train.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;buildDataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;imputer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;impute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;scaler&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;standardize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


    &lt;span class="n"&gt;test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;loadData&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;test.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;tmp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;buildDataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;tmp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;impute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;imputer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;tmp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;standardize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;scaler&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;什么？你问我为什么要把 imputer 和 scaler 反复使用，恭喜你终于找到了本章最大的重点，&lt;strong&gt;&lt;em&gt;作用于训练集上的改变要完全一致，不打任何折扣的作用在所有数据上&lt;/em&gt;&lt;/strong&gt;，否则你训练出来的模型将不能适用于任何地方，在别的地方一无是处，你整个工作都白费了。&lt;/p&gt;
&lt;p&gt;啊，你说这么做太麻烦了，我也这么觉得的。索性 scikit-learn 提供了Pipeline功能，我们可以把读进数据后的所有操作打包成一个管线，这么做就方便多了。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.pipeline&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Pipeline&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;raw&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;loadData&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;train.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;buildDataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;loadData&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;test.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;tmp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;buildDataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;pipel&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Pipeline&lt;/span&gt;&lt;span class="p"&gt;([(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;impute&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;pp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Imputer&lt;/span&gt;&lt;span class="p"&gt;()),(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;standardize&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;pp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;StandardScaler&lt;/span&gt;&lt;span class="p"&gt;())])&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pipel&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pipel&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;好了，数据就准备到这里，我们进入下一步。特征提取。&lt;/p&gt;
&lt;p&gt;Written with &lt;a href="https://stackedit.io/"&gt;StackEdit&lt;/a&gt;.&lt;/p&gt;</summary><category term="Machine Learning"></category><category term="Scikit-Learn"></category><category term="python"></category></entry><entry><title>玩转Kaggle之Titanic(1):这货到底在问什么？</title><link href="https://zwy1135.github.io/wan-zhuan-kagglezhi-titanic1zhe-huo-dao-di-zai-wen-shi-yao.html" rel="alternate"></link><updated>2014-08-13T00:00:00+08:00</updated><author><name>Zeng Wenyuan</name></author><id>tag:zwy1135.github.io,2014-08-13:wan-zhuan-kagglezhi-titanic1zhe-huo-dao-di-zai-wen-shi-yao.html</id><summary type="html">&lt;p&gt;接上回。&lt;/p&gt;
&lt;p&gt;我们在上回扯了一大堆淡，终于把要做的题目定下来了，从这一篇开始就要上干货了，可喜可贺，喜大普奔。。。。。那么，我们究竟要做点什么呢？&lt;/p&gt;
&lt;p&gt;多家游戏界的大佬们啊用自己的一款款大作告诉我们，关卡开始前一掉要先来一段任务简报舒缓下心情。我们来看看这里面有什么内容。&lt;/p&gt;
&lt;p&gt;在一望无际的大海上，马上就要沉掉的不沉之舟泰坦尼克载着杰克和罗丝飘荡着缓缓地向着目的地驶去，突然，航路上一座冰山狠狠地撞了过来。。。。。。&lt;/p&gt;
&lt;p&gt;喂喂，快醒醒，我们不是在拍电影啊，看重点。&lt;/p&gt;
&lt;p&gt;在那次事故中，2224人里面有1502人当场就被GM封号，永远的离开了地球Online。去的人自然是去了，活下来的人更要想尽办法继续活下去。那么，问题出现了，&lt;strong&gt;问&lt;/strong&gt;：在沉船事故发生这种&lt;strong&gt;状况&lt;/strong&gt;下(是沉船状况下，不是空难状况下。此点的重要性会在下一篇讲到)，某个人是不是可能&lt;strong&gt;活下来&lt;/strong&gt;？&lt;/p&gt;
&lt;p&gt;Bingo！ 各位看官，看到这里要打起精神来了，我们被问的是这个人&lt;strong&gt;&lt;em&gt;*是否&lt;/em&gt;&lt;/strong&gt;*能活下来而不是这个人能活&lt;strong&gt;&lt;em&gt;多长&lt;/em&gt;&lt;/strong&gt;，也不是这个人对自己活下来这件事的满意&lt;strong&gt;&lt;em&gt;程度&lt;/em&gt;&lt;/strong&gt;是多少。重点出来了：我们要求得的结果是&lt;strong&gt;&lt;em&gt;是&lt;/em&gt;&lt;/strong&gt;或者&lt;strong&gt;&lt;em&gt;否&lt;/em&gt;&lt;/strong&gt;，换句话说，我们要求得的&lt;strong&gt;&lt;em&gt;结果是&lt;/em&gt;&lt;/strong&gt;一个&lt;strong&gt;&lt;em&gt;离散的&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;类别(Categories)&lt;/em&gt;&lt;/strong&gt;，这就表明了我们现在要做的要么是聚类，要么就是分类了。&lt;/p&gt;
&lt;p&gt;再看看我们现在有什么。我们现在有一个登记表，上面记录着乘客票价，仓位等信息以及他们是否幸存。也就是说，我们&lt;strong&gt;&lt;em&gt;现有&lt;/em&gt;&lt;/strong&gt;一份&lt;strong&gt;&lt;em&gt;标记好&lt;/em&gt;&lt;/strong&gt;乘客是否生存的数据。&lt;/p&gt;
&lt;p&gt;现有有标记数据，求离散类别的结果，这就是一个妥妥的分类问题没跑了。&lt;/p&gt;
&lt;p&gt;谨记这一点，我们进入&lt;strong&gt;数据准备&lt;/strong&gt;阶段。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;我是愉快的分割线&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;上面闲扯了那么多有的没的东西，现在我来告诉大家一个简单的判断问题的方法：&lt;/p&gt;
&lt;p&gt;根据现有数据和所求的结果来判断：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;现有有标记数据，求离散的类别(Categories)结果   -&amp;gt; 分类(Classification)问题&lt;/p&gt;
&lt;p&gt;现有有标记数据，求连续的数值(Numerical)结果    -&amp;gt; 回归(Regression)问题&lt;/p&gt;
&lt;p&gt;现有无标记数据，求离散的类别结果               -&amp;gt; 聚类(Clustering)问题&lt;/p&gt;
&lt;p&gt;现有无标记数据，求连续的数值结果               -&amp;gt; 我也不知道什么问题&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;此外还有根据现象发生的时间求现象间的因果关系之类的问题等等其他问题，因为博主现在也不太熟悉。这里也不作细说。&lt;/p&gt;
&lt;p&gt;Written with &lt;a href="https://stackedit.io/"&gt;StackEdit&lt;/a&gt;.&lt;/p&gt;</summary><category term="Machine Learning"></category><category term="Scikit-Learn"></category><category term="python"></category></entry><entry><title>玩转Kaggle之Titanic(0):开坑宣言</title><link href="https://zwy1135.github.io/wan-zhuan-kagglezhi-titanic0kai-keng-xuan-yan.html" rel="alternate"></link><updated>2014-08-12T00:00:00+08:00</updated><author><name>Zeng Wenyuan</name></author><id>tag:zwy1135.github.io,2014-08-12:wan-zhuan-kagglezhi-titanic0kai-keng-xuan-yan.html</id><summary type="html">&lt;p&gt;观众朋友们，欢迎收看最新一期的Kaggle挑战赛娱乐实况解说。由于博主也是现学现卖，在本系列内容中，你们将不可避免的看到：博主犯二，代码错漏，方法行不通，前面全部推到重来等内容。遇到这种情况，其立即保护好自己的双眼并果断右上方点×。不过，如果有真的猛士能忍下这些糟糕的内容指点我一下的话，请在下方留言，谢谢。&lt;/p&gt;
&lt;p&gt;废话不多说，我们开始吧。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;先进一段背景。&lt;/p&gt;
&lt;h3&gt;什么是Kaggle？&lt;/h3&gt;
&lt;p&gt;Kaggle是由Predictive modelling公司于2010年四月推出的众包数据分析与竞赛平台。企业可以在上面以竞赛的形式发布自己数据分析的问题，寻求解决方法，而全世界任何人都可以注册，参与其中，提出自己的想法和解决方案，平台实时给予反馈结果，可以在排行榜中看到自己的位置。(引自&lt;a href="http://blog.sina.com.cn/s/blog_648ca2350101ho9p.html"&gt;这里&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;经过这么多年的发展，Kaggle在机器学习与数据分析领域中极具影响力，他的地位就像吉米多维奇习题集之于数学分析或者POJ之于ACM竞赛。&lt;/p&gt;
&lt;p&gt;总之对于对机器学习和数据科学感兴趣的同学，有事没事上去刷一刷题，勾搭一下各路大牛还是能起到活泼身心治疗失眠的作用的。&lt;/p&gt;
&lt;h3&gt;什么是Scikit-Learn？&lt;/h3&gt;
&lt;p&gt;Scikit-Learn是这一系列文章中主要用到的工具。它是Python下的一个重要的机器学习库，它实现了大部分经典的机器学习算法而且提供了方便使用又风格统一的接口。近年来使用人数不断上升，大有成为行业标准的趋势。更重要的是这是博主唯一会用的机器学习库，所以只能用它来卖弄一下了。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;了解完背景之后我们就可以进正题了。&lt;/p&gt;
&lt;p&gt;点开Kaggle的competitions页面，上面还是有很多正在进行的比赛的。不过，对于我们这种刚入门的新手肯定是不能上来就挑战那些难得吓人大神专用关卡。我们还是来看看101新手村里面有什么教学关卡好玩：&lt;/p&gt;
&lt;p&gt;手写数字识别、脸部关键点提取。。。图像识别啊，单是从图像里提取特征就够写一整系列的东西了，我们这些刚入门的渣渣就不要选这么高难度的东西了。&lt;/p&gt;
&lt;p&gt;Julia入门，Scikit-Learn官方教程。。。。这种连背景都没有就啪一段数据上来的东西，还让不让人愉快的玩耍了。&lt;/p&gt;
&lt;p&gt;泰坦尼克号上哪些人会是幸存者？这个问题好玩，就决定是你了皮卡，呃不，Titanic！。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;问题选定了，我们就可以进入机器学习问题的标准流程：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;考察问题 &amp;gt; 准备数据 &amp;gt; 提取特征 &amp;gt; 选定模型 &amp;gt; 确定参数 &amp;gt; 实施并评估效果&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;可以看出，上面这六个步骤环环相扣，每一步都构成了后面步骤的基础，一步都马虎不得。&lt;/p&gt;
&lt;p&gt;在接下来的一系列文章里，我会带着大家一步步走完这整个流程。敬请期待。&lt;/p&gt;
&lt;p&gt;Written with &lt;a href="https://stackedit.io/"&gt;StackEdit&lt;/a&gt;.&lt;/p&gt;</summary><category term="Machine Learning"></category><category term="Scikit-Learn"></category><category term="python"></category></entry><entry><title>Hello World！</title><link href="https://zwy1135.github.io/hello-world.html" rel="alternate"></link><updated>2014-08-11T00:00:00+08:00</updated><author><name>Zeng Wenyuan</name></author><id>tag:zwy1135.github.io,2014-08-11:hello-world.html</id><summary type="html">&lt;p&gt;断断续续折腾了好几天，终于是吧这个博客弄起来了。&lt;/p&gt;
&lt;p&gt;用pelican配的github pages，不得不说python2在windows下的编码问题真是令人蛋碎啊。从安装开始就不停的报unicodedecodeError。差点把我搞疯了。这是逼我转投python3的节奏么。&lt;/p&gt;
&lt;p&gt;不管怎样，问题终究是解决了。解法是在每一个报错的文件里加上&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sys&lt;/span&gt;
&lt;span class="nb"&gt;reload&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setdefaultencoding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getfilesystemencoding&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;不小心误入这里的朋友也许可以用上。&lt;/p&gt;
&lt;p&gt;最后感谢一下frantic1048提供的pelican使用&lt;a href="http://frantic1048.com/bo-ke-dan-sheng-ji-ji-yu-githubpelicanchuang-jian-bo-ke-de-zheng-ge-guo-cheng.html"&gt;教程&lt;/a&gt;。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Written with &lt;a href="https://stackedit.io/"&gt;StackEdit&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;</summary><category term="杂物"></category><category term="随笔"></category><category term="python"></category></entry></feed>